<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Wall Designer Tool - CS566 Final Project</title>
    <link rel="stylesheet" href="styles.css" />
    <!-- Google Fonts for better typography -->
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&family=Montserrat:wght@600;700&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <header>
      <div class="container">
        <h1>Wall Designer Tool</h1>
        <p class="subtitle">CS566 Computer Vision (Fall 2025S) Final Project </p>
        <div class="team-members" style="margin-top: 0.5rem; margin-bottom: 2.5rem;">
          <p>
            <strong>Bhinu Puvva</strong> - puvvala@wisc.edu | 
            <strong>Bala Shukla</strong> - shukla35@wisc.edu | 
            <strong>Rain Jiayu Sun</strong> - jsun424@wisc.edu
          </p>
        </div>
        <nav>
          <ul>
            <li><a href="#motivation">Motivation</a></li>
            <li><a href="#approach">Approach</a></li>
            <li><a href="#implementation">Implementation</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#discussion">Discussion</a></li>
            <li><a href="#video">Video</a></li>
          </ul>
        </nav>
      </div>
    </header>

    <main class="container">
      <!-- Video -->
      <section id="video">
        <h2>Presentation Video</h2>
        <div class="content-block center-text">

          <div class="video-container">
            <iframe
              src="https://www.youtube.com/embed/28DcFrfub-0"
              title="Wall Designer Tool Presentation"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen
            ></iframe>
          </div>
        </div>
      </section>

      <!-- Motivation -->
      <section id="motivation">
        <h2>Motivation</h2>
        <div class="content-block">
          <p
            style="
              background-color: #e8f4f8;
              padding: 10px;
              border-left: 4px solid #3498db;
              margin-bottom: 20px;
            "
          >
            <strong>See also:</strong> Check out our
            <a href="midterm.html"><strong>Midterm Project Report</strong></a> 
            for details on our team, initial experiments, and methodology
            evolution.
          </p>
          <h3>Problem Statement</h3>
          <p>
            Modern AR and virtual-staging tools need to replace wall textures
            (paint, wallpapers) in a realistic way. But this requires accurately
            identifying planar surfaces from a single indoor image and applying
            textures with correct perspective, geometry, and lighting.
          </p>

          <h3>Research Question</h3>
          <p>
            Can modern computer-vision methods reliably segment planar surfaces
            from a single image to enable realistic, perspective-correct texture
            transfer?
          </p>

          <h3>Why this matters</h3>
          <ul>
            <li>
              AR interior design apps demand instant wall replacement - not a
              slow manual process
            </li>
            <li>
              Traditional CV methods fail in messy, cluttered indoor scenes
            </li>
            <li>
              Realistic overlays require knowing where the wall is, its shape,
              and perspective
            </li>
            <li>
              A single-image solution enables scalable deployment: mobile apps,
              real estate, home-renovation tools
            </li>
            <strong>Goal:</strong>
            Build a pipeline that works robustly, fast, and without a lot of
            manual work like photoshop or manual segmentation
          </ul>

          <h3>Gaps in Existing Research</h3>
          <p>
            Despite strong progress in semantic segmentation, critical gaps
            remain:
          </p>
          <ul>
            <li>
              <strong>Fragmented Workflows:</strong> Existing research treats
              segmentation, geometry extraction, and texture overlay as separate
              problems, with no unified, reliable pipeline that connects all
              three steps for planar surfaces.
            </li>
            <li>
              <strong>Limited Practicality:</strong> Many AR systems depend on
              depth sensors or assume clean, minimal environments, which makes
              them unreliable in real indoor scenes with clutter, occlusions,
              varied lighting, and complex wall textures.
            </li>
          </ul>
          <p>Our work fills this gap by:</p>
          <ul>
            <li>
              Benchmarking non-semantic vs. semantic segmentation to identify
              optimal wall isolation methods.
            </li>
            <li>
              Building a complete monocular pipeline that handles occlusion via
              semantic masking and preserves lighting via intrinsic blending.
            </li>
          </ul>
        </div>
      </section>

      <!-- Approach -->
      <section id="approach">
        <h2>Approach</h2>
        <div class="content-block">
          <h3>Methodology Overview</h3>
          <p>
            We propose a three-stage pipeline to solve this problem:
            <strong
              >Segmentation, Refinement (Splitting), and
              Blending/Mapping.</strong
            >
          </p>

          <div class="step-container">
            <div class="step-text">
              <h3>1. Segmentation</h3>
              <p>
                Initially we tried using simple edge detection (Hough Transform)
                and color based clustering. Both these failed for 2 main reasons
                (noise and a lack of semantic understanding).
              </p>
              <p>
                We realized we needed a deep learning model with semantic
                understanding. So we had a couple of options:
              </p>
              <ul>
                <li>Mask2Former: Transformer-based, Swin backbone</li>
                <li>OneFormer: Universal Segmentation Model</li>
                <li>SegFormer/ NYU: Efficient Alternatives</li>
              </ul>
            </div>
            <div class="step-image">
              <figure style="margin: 0; text-align: center">
                <img
                  src="images/Segmentation.png"
                  alt="Segmentation Comparison"
                  style="
                    width: 100%;
                    border-radius: 8px;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
                  "
                />
                <figcaption
                  style="margin-top: 10px; font-style: italic; color: #666"
                >
                  Fig 1. Segmentation Comparison
                </figcaption>
              </figure>
            </div>
          </div>

          <div class="step-container">
            <div class="step-text">
              <h3>2. Splitting Method</h3>
              <p>
                We explored image-based methods like Canny Edge + Hough
                Transform to detect vertical dividers and Vertical Projection
                (summing gradients) to find wall breaks, but these were too
                sensitive to noise (like wallpaper patterns) or failed in dark
                corners.
              </p>
              <p>
                We settled on
                <strong
                  >Geometric Contour Analysis (specifically
                  CeilingKinkSplitting)</strong
                >. The rationale is that the ceiling line is the most reliable
                feature in a room, whereas the floor is often occluded by
                furniture; by detecting geometric "kinks" (corners) in the
                ceiling profile, we can reliably separate walls and
                mathematically infer the floor line to create clean trapezoidal
                planes, even when the actual floor boundary is hidden.
              </p>
            </div>
            <div class="step-image">
              <figure style="margin: 0; text-align: center">
                <img
                  src="images/splitting.png"
                  alt="Splitting Image"
                  style="
                    width: 100%;
                    border-radius: 8px;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
                  "
                />
                <figcaption
                  style="margin-top: 10px; font-style: italic; color: #666"
                >
                  Fig 2. Splitting Overview
                </figcaption>
              </figure>
            </div>
          </div>

          <div class="step-container">
            <div class="step-text">
              <h3>3. Mapping Method</h3>
              <p>
                We initially tried Standard Homography, which warped the texture
                to fit the detected polygon, but this often caused "twisting"
                artifacts on slanted walls (due to automatic point reordering)
                and painted over objects.
              </p>
              <p>
                We settled on <strong>Masked Perspective Mapping</strong>. This
                approach is superior because it decouples geometry from
                visibility: it uses the Geometric Polygon strictly to define the
                correct perspective warp (trusting the specific vertex order to
                prevent twists) and then applies the Semantic Mask as a "cookie
                cutter" to handle occlusions (preventing texture from spilling
                onto lamps or plants), finally using Multiply Blending to
                preserve the room's natural shadows and lighting.
              </p>
            </div>
            <div class="step-image">
              <figure style="margin: 0; text-align: center">
                <img
                  src="images/mapping.png"
                  alt="Mapped Image"
                  style="
                    width: 100%;
                    border-radius: 8px;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
                  "
                />
                <figcaption
                  style="margin-top: 10px; font-style: italic; color: #666"
                >
                  Fig 3. Mapping/Blending
                </figcaption>
              </figure>
            </div>
          </div>

          <h3>Putting it all together</h3>
          <div class="placeholder-image large">
            <img
              src="images/Altogether.png"
              alt="Complete System Pipeline"
              style="
                width: 100%;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
              "
            />
            <p><em>Fig 4. Complete System Pipeline</em></p>
          </div>

          <h3>Dataset</h3>
          <p>
            The system was developed and tested using the
            <strong>ADE20K</strong> dataset for validation, along with
            custom-captured indoor scenes ("OurSet") to test real-world
            robustness.
          </p>
        </div>
      </section>

      <!-- Implementation -->
      <section id="implementation">
        <h2>Implementation</h2>
        <div class="content-block">
          <h3>Tech Stack</h3>
          <ul class="tech-stack">
            <li><strong>Language:</strong> Python 3.10+</li>
            <li>
              <strong>Deep Learning:</strong> PyTorch, Hugging Face Transformers
              (for Mask2Former)
            </li>
            <li>
              <strong>Computer Vision:</strong> OpenCV (cv2) for geometric
              processing and image manipulation
            </li>
            <li>
              <strong>Data Handling:</strong> NumPy, PIL, Hugging Face Datasets
            </li>
            <li><strong>Visualization:</strong> Matplotlib</li>
          </ul>

          <h3>Engineering Challenges</h3>
          <p>
            A major implementation challenge was the
            <strong>"Corner Problem"</strong>. Semantic segmentation models
            treat all walls as a single "wall" class, ignoring geometry. We
            engineered a robust splitting algorithm that:
          </p>
          <ol>
            <li>Filters edges to find only those internal to the wall mask.</li>
            <li>
              Filters Hough lines by angle to strictly target vertical wall
              intersections.
            </li>
            <li>
              Approximates the resulting blobs into clean 4-point polygons for
              homography calculation.
            </li>
          </ol>
        </div>
      </section>

      <!-- Results -->
      <section id="results">
        <h2>Results, Benchmark, and Evaluation</h2>
        <div class="content-block">
          <h3>Benchmarking on Segmentation</h3>
          <h4>Why we benchmark on Segmentation?</h4>
          <p>Reason here</p>
          <h4>Below are the segmentation Benchmarking on 4 models: Mask2Former, OneFormer, SegFormer, and NYU</h4>
          <p>what are the 4 models? include a link to each, and a brief explanation why we chose these</p>
          
        </div>
        <div class="content-block">
          <h3>Qualitative Results</h3>
          <h4>Box Plot Comparison of the 4 Models</h4>
          <div class="boxplot-container">
            <figure class="boxplot-figure">
              <img
                src="images/box_plot_comparison.png"
                alt="Box Plot Comparison: Mask2Former, OneFormer, SegFormer, and NYU"
                class="boxplot-image"
              />
              <figcaption class="boxplot-caption">
                Fig. Box Plot Comparison: Performance metrics across Mask2Former, OneFormer, SegFormer, and NYU models
              </figcaption>
            </figure>
          </div>
          <p><strong>Discussion: </strong>we can see from the graph that the performance of Mask2Former and OneFormer and similar, both slightly outperforms the SegFormer and the NYU model</p>
          <hr>
          <br>
          <p><strong>Statistics Summary of the 4 Models</strong></p>
          <div class="boxplot-container">
            <figure class="boxplot-figure">
              <img
                src="images/statistics_summary.png"
                alt="Statistics Summary: Mask2Former, OneFormer, SegFormer, and NYU"
                class="boxplot-image"
              />
              <figcaption class="boxplot-caption">
                Fig. Box Plot Comparison: Performance metrics across Mask2Former, OneFormer, SegFormer, and NYU models
              </figcaption>
            </figure>
          </div>
          <p><strong>Discussion:</strong> We could see from this graph that the OneFormer cost more computation/time than any other models. This is one reason that we eventually chose the Mask2Former as the segmentation model.</p>
          <hr>
          <br>

          <div class="gallery">
            <div class="gallery-item">
              <div class="placeholder-image large">
                <p>[PLACEHOLDER: Result Image Set 1]</p>
                <p>
                  (Original Image | Segmentation Mask | Split Polygons | Final
                  Output)
                </p>
              </div>
              <p class="caption">
                Fig 2. Texture transfer on a complex living room scene.
              </p>
            </div>
            <div class="gallery-item">
              <div class="placeholder-image large">
                <p>[PLACEHOLDER: Result Image Set 2]</p>
                <p>
                  (Original Image | Segmentation Mask | Split Polygons | Final
                  Output)
                </p>
              </div>
              <p class="caption">
                Fig 3. Handling multiple adjacent walls in a hallway.
              </p>
            </div>
          </div>

          <h3>Quantitative Evaluation</h3>
          <p>
            We evaluated our segmentation and splitting accuracy against
            manually annotated ground truth.
          </p>

          <table class="results-table">
            <thead>
              <tr>
                <th>Method</th>
                <th>IoU (Intersection over Union)</th>
                <th>Wall Detection Accuracy</th>
                <th>Inference Time (s)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Baseline (Color Clustering)</td>
                <td>[PLACEHOLDER]</td>
                <td>[PLACEHOLDER]</td>
                <td>[PLACEHOLDER]</td>
              </tr>
              <tr>
                <td><strong>Ours (Mask2Former + Refiner)</strong></td>
                <td><strong>[PLACEHOLDER]</strong></td>
                <td><strong>[PLACEHOLDER]</strong></td>
                <td><strong>[PLACEHOLDER]</strong></td>
              </tr>
            </tbody>
          </table>

          <h3>Comparison</h3>
          <div class="placeholder-image">
            <p>[PLACEHOLDER: Visual comparison with baseline method]</p>
            <p><em>Fig 4. Comparison of Our Method vs. Naive Homography</em></p>
          </div>
        </div>
      </section>

      <!-- Discussion -->
      <section id="discussion">
        <h2>Discussion</h2>
        <div class="content-block">
          <h3>Findings & Implications</h3>
          <p>
            Our results demonstrate that while modern semantic segmentation is
            powerful, it lacks geometric understanding. The hybrid approach of
            <strong
              >Deep Learning (for semantics) + Classical CV (for
              geometry)</strong
            >
            proved superior to using either approach in isolation.
          </p>

          <h3>Limitations & Challenges</h3>
          <ul>
            <li>
              <strong>Occlusions:</strong> Furniture blocking the wall-floor
              boundary can sometimes confuse the polygon approximation.
            </li>
            <li>
              <strong>Complex Geometry:</strong> Non-planar walls (curved
              surfaces) are currently modeled as flat planes.
            </li>
          </ul>

          <h3>Future Work</h3>
          <p>
            Future extensions could include implementing
            <strong>PlaneRCNN</strong> for 3D-aware segmentation or integrating
            a monocular depth estimation model (like MiDaS) to handle occlusions
            more accurately.
          </p>
        </div>
      </section>
    </main>

    <footer>
      <div class="container">
        <p>&copy; 2025 Wall Designer Tool. CS566 Final Project.</p>
      </div>
    </footer>
  </body>
</html>
