<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Wall Designer Tool - CS566 Final Project</title>
    <link rel="stylesheet" href="styles.css" />
    <!-- Google Fonts for better typography -->
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&family=Montserrat:wght@600;700&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <header>
      <div class="container">
        <h1>Enabling Perspective Texture Transfer</h1>
        <p class="subtitle">CS566 Computer Vision (Fall 2025) Final Project</p>
        <div
          class="team-members"
          style="margin-top: 0.5rem; margin-bottom: 1.5rem"
        >
          <p>
            <strong>Bhinu Puvva</strong> - puvvala@wisc.edu |
            <strong>Bala Shukla</strong> - shukla35@wisc.edu |
            <strong>Rain Jiayu Sun</strong> - jsun424@wisc.edu
          </p>
        </div>
        <div style="text-align: center; margin-bottom: 2.5rem;">
          <a href="https://github.com/RainJiayuSun-star/WallDesignerTool" target="_blank" style="display: inline-flex; align-items: center; text-decoration: none; color: #333; background-color: #f6f8fa; padding: 5px 10px; border: 1px solid #d0d7de; border-radius: 6px; font-weight: 600; font-size: 0.9rem;">
            <svg height="20" aria-hidden="true" viewBox="0 0 16 16" version="1.1" width="20" data-view-component="true" style="margin-right: 5px;">
              <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
            </svg>
            View on GitHub
          </a>
        </div>
        <nav>
          <ul>
            <li><a href="#motivation">Motivation</a></li>
            <li><a href="#approach">Approach</a></li>
            <li><a href="#implementation">Implementation</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#discussion">Discussion</a></li>
            <li><a href="#video">Video</a></li>
          </ul>
        </nav>
      </div>
    </header>

    <main class="container">
      <!-- Video -->
      <section id="video">
        <h2>Presentation Video</h2>
        <div class="content-block center-text">
          <div class="video-container">
            <iframe
              src="https://www.youtube.com/embed/28DcFrfub-0"
              title="Wall Designer Tool Presentation"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen
            ></iframe>
          </div>
        </div>
      </section>

      <!-- Motivation -->
      <section id="motivation">
        <h2>Motivation</h2>
        <div class="content-block">
          <p
            style="
              background-color: #e8f4f8;
              padding: 10px;
              border-left: 4px solid #3498db;
              margin-bottom: 20px;
            "
          >
            <strong>See also:</strong> Check out our
            <a href="midterm.html"><strong>Midterm Project Report</strong></a> 
            for details on our team, initial experiments, and methodology
            evolution.
          </p>
          <h3>Problem Statement</h3>
          <p>
            Modern AR and virtual-staging tools need to replace wall textures
            (paint, wallpapers) in a realistic way. But this requires accurately
            identifying planar surfaces from a single indoor image and applying
            textures with correct perspective, geometry, and lighting.
          </p>

          <h3>Research Question</h3>
          <p>
            Can modern computer-vision methods reliably segment planar surfaces
            from a single image to enable realistic, perspective-correct texture
            transfer?
          </p>

          <h3>Why this matters</h3>
          <ul>
            <li>
              AR interior design apps demand instant wall replacement - not a
              slow manual process
            </li>
            <li>
              Traditional CV methods fail in messy, cluttered indoor scenes
            </li>
            <li>
              Realistic overlays require knowing where the wall is, its shape,
              and perspective
            </li>
            <li>
              A single-image solution enables scalable deployment: mobile apps,
              real estate, home-renovation tools
            </li>
            <strong>Goal:</strong>
            Build a pipeline that works robustly, fast, and without a lot of
            manual work like photoshop or manual segmentation
          </ul>

          <h3>Gaps in Existing Research</h3>
          <p>
            Despite strong progress in semantic segmentation, critical gaps
            remain:
          </p>
          <ul>
            <li>
              <strong>Fragmented Workflows:</strong> Existing research treats
              segmentation, geometry extraction, and texture overlay as separate
              problems, with no unified, reliable pipeline that connects all
              three steps for planar surfaces.
            </li>
            <li>
              <strong>Limited Practicality:</strong> Many AR systems depend on
              depth sensors or assume clean, minimal environments, which makes
              them unreliable in real indoor scenes with clutter, occlusions,
              varied lighting, and complex wall textures.
            </li>
          </ul>
          <p>Our work fills this gap by:</p>
          <ul>
            <li>
              Benchmarking non-semantic vs. semantic segmentation to identify
              optimal wall isolation methods.
            </li>
            <li>
              Building a complete monocular pipeline that handles occlusion via
              semantic masking and preserves lighting via intrinsic blending.
            </li>
          </ul>
        </div>
      </section>

      <!-- Approach -->
      <section id="approach">
        <h2>Approach</h2>
        <div class="content-block">
          <h3>Methodology Overview</h3>
          <p>
            We propose a three-stage pipeline to solve this problem:
            <strong
              >Segmentation, Refinement (Splitting), and
              Blending/Mapping.</strong
            >
          </p>

          <div class="step-container">
            <div class="step-text">
              <h3>1. Segmentation</h3>
              <p>
                Initially we tried using simple edge detection (Hough Transform)
                and color based clustering. Both these failed for 2 main reasons
                (noise and a lack of semantic understanding).
              </p>
              <p>
                We realized we needed a deep learning model with semantic
                understanding. So we had a couple of options:
              </p>
              <ul>
                <li>Mask2Former: Transformer-based, Swin backbone</li>
                <li>OneFormer: Universal Segmentation Model</li>
                <li>SegFormer/ NYU: Efficient Alternatives</li>
              </ul>
            </div>
            <div class="step-image">
              <figure style="margin: 0; text-align: center">
                <img
                  src="images/Segmentation.png"
                  alt="Segmentation Comparison"
                  style="
                    width: 100%;
                    border-radius: 8px;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
                  "
                />
                <figcaption
                  style="margin-top: 10px; font-style: italic; color: #666"
                >
                  Fig 1. Segmentation Comparison
                </figcaption>
              </figure>
            </div>
          </div>

          <div class="step-container">
            <div class="step-text">
          <h3>2. Splitting Method</h3>
          <p>
            We initially moved from local edge detection to 
            <strong>Vertical Projection Analysis</strong> (summing gradients to find structural peaks), 
            which successfully reduced over-segmentation compared to Canny edges. However, it still struggled with 
            non-vertical wall boundaries.
          </p>
          <p>
            Our final breakthrough was
            <strong
              >Geometric Contour Analysis (specifically
              CeilingKinkSplitting)</strong
            >. The rationale is that the ceiling line is the most reliable
            feature in a room. By analyzing the vector profile of the segmentation mask's upper boundary, 
            we could detect geometric "kinks" (corners) directly. This allowed us to filter out noise more 
            robustly than pixel-grid methods and mathematically infer the floor line to create clean trapezoidal
            planes.
          </p>
            </div>
            <div class="step-image">
              <figure style="margin: 0; text-align: center">
                <img
                  src="images/splitting.png"
                  alt="Splitting Image"
                  style="
                    width: 100%;
                    border-radius: 8px;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
                  "
                />
                <figcaption
                  style="margin-top: 10px; font-style: italic; color: #666"
                >
                  Fig 2. Splitting Overview
                </figcaption>
              </figure>
            </div>
          </div>

          <div class="step-container">
            <div class="step-text">
              <h3>3. Mapping Method</h3>
              <p>
                We initially tried Standard Homography, which warped the texture
                to fit the detected polygon, but this often caused "twisting"
                artifacts on slanted walls (due to automatic point reordering)
                and painted over objects.
              </p>
              <p>
                We settled on <strong>Masked Perspective Mapping</strong>. This
                approach is superior because it decouples geometry from
                visibility: it uses the Geometric Polygon strictly to define the
                correct perspective warp (trusting the specific vertex order to
                prevent twists) and then applies the Semantic Mask as a "cookie
                cutter" to handle occlusions (preventing texture from spilling
                onto lamps or plants), finally using Multiply Blending to
                preserve the room's natural shadows and lighting.
              </p>
            </div>
            <div class="step-image">
              <figure style="margin: 0; text-align: center">
                <img
                  src="images/mapping.png"
                  alt="Mapped Image"
                  style="
                    width: 100%;
                    border-radius: 8px;
                    box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
                  "
                />
                <figcaption
                  style="margin-top: 10px; font-style: italic; color: #666"
                >
                  Fig 3. Mapping/Blending
                </figcaption>
              </figure>
            </div>
          </div>

          <h3>Putting it all together</h3>
          <div class="placeholder-image large">
            <img
              src="images/Altogether.png"
              alt="Complete System Pipeline"
              style="
                width: 100%;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
              "
            />
            <p><em>Fig 4. Complete System Pipeline</em></p>
          </div>

          <h3>Dataset</h3>
          <p>
            The system was developed and tested using the
            <strong>ADE20K</strong> dataset for validation, along with
            custom-captured indoor scenes ("OurSet") to test real-world
            robustness.
          </p>
          <p>
            <em>Note on Dataset Selection:</em> We shifted our primary evaluation dataset from ScanNet (proposed in the midterm) to ADE20K. 
            While ScanNet is excellent for 3D tasks, ADE20K provides high-quality 2D semantic annotations that are more directly aligned with our single-image segmentation goal. 
            Furthermore, state-of-the-art models like Mask2Former are natively optimized for the ADE20K benchmark, allowing for more standardized comparisons.
          </p>
        </div>
      </section>

      <!-- Implementation -->
      <section id="implementation">
        <h2>Implementation</h2>
        <div class="content-block">
          <h3>Tech Stack</h3>
          <ul class="tech-stack">
            <li><strong>Language:</strong> Python 3.10+</li>
            <li>
              <strong>Deep Learning:</strong> PyTorch, Hugging Face Transformers
            </li>
            <li><strong>Models:</strong> Mask2Former, OneFormer, SegFormer</li>
            <li>
              <strong>Computer Vision:</strong> OpenCV (cv2) for geometric
              processing, edge detection, and homography
            </li>
            <li>
              <strong>Data Pipeline:</strong> Hugging Face Datasets (streaming
              ADE20K), NumPy, Pillow
            </li>
            <li><strong>Visualization:</strong> Matplotlib</li>
          </ul>

          <h3>Engineering Challenges</h3>
          <p>
            Bridging the gap between
            <strong>semantic understanding</strong> (what is a wall?) and
            <strong>geometric understanding</strong> (where is the corner?) was
            our primary challenge.
          </p>

          <h4>1. The "Corner Problem" (Geometric Splitting)</h4>
          <p>
            Semantic models return a single binary blob for all walls in a room.
            To texture them correctly, we must split this blob into distinct
            planes.
            <br />
            <em>Failed Approach:</em> Standard Canny Edge Detection + Hough
            Transform was too sensitive to texture noise (e.g., stripes on
            wallpaper) and failed in dark corners.
            <br />
            <em>Our Solution:</em> <strong>Ceiling Kink Analysis</strong>. Since
            ceilings are rarely occluded compared to floors, we analyze the
            upper contour of the wall mask. We detect sharp geometric changes
            ("kinks") in the ceiling line to identify corners, then
            mathematically project these down to infer the floor boundary,
            creating clean 3D-aware trapezoids even when the floor is hidden by
            a couch.
          </p>
          <p>
            <em>Robustness Detail:</em> To prevent artifacts where vertical wall edges were mistaken for ceilings (causing diagonal texture smears), we implemented <strong>Adaptive Slope Filtering</strong>. This rejects high-gradient segments from the ceiling path, ensuring only true horizontal-ish spans are textured.
          </p>

          <h4>2. Perspective "Twisting" Artifacts</h4>
          <p>
            <em>Challenge:</em> Standard homography techniques often "twist" the
            texture on slanted walls (e.g., attic ceilings or angled hallways)
            because they blindly sort vertices by coordinate position (Top-Left,
            Top-Right, etc.).
            <br />
            <em>Solution:</em> We discovered that re-sorting was the problem. We
            implemented <strong>Slope-Aware Ordering</strong> derived directly
            from our "Ceiling Kink" analysis. By strictly maintaining the
            sequence of the detected upper contour, we ensured vertices always
            matched the physical vanishing points without artificial reordering,
            eliminating twisting even on steep angles.
          </p>

          <h4>3. Occlusion Handling</h4>
          <p>
            We developed a
            <strong>Masked Perspective Mapping</strong> technique. We compute
            the perspective warp using the <em>geometric trapezoid</em> (perfect
            perspective) but apply the texture only within the
            <em>semantic mask</em> (pixel-perfect boundaries). This ensures the
            wallpaper has the correct 3D slant but doesn't accidentally paint
            over a lamp or plant standing in front of the wall.
          </p>
        </div>
      </section>

      <!-- Results -->
      <section id="results">
        <h2>Results, Benchmark, and Evaluation</h2>
        <div class="content-block">
          <h3>Benchmarking on Segmentation</h3>
          <h4>Why benchmark Segmentation?</h4>
          <p>
            Accurate segmentation is the foundation of our pipeline. If the
            model fails to capture the precise boundary between the wall and the
            ceiling, the subsequent geometric analysis (Kink Detection) will
            fail, leading to slanted or distorted textures. We needed to find
            the optimal balance between
            <strong>boundary precision</strong> (IoU) and
            <strong>inference speed</strong>.
          </p>

          <h4>Metrics Used</h4>
          <ul>
            <li>
              <strong>IoU (Intersection over Union):</strong> Measures the
              degree of overlap between the predicted segmentation and the
              ground truth.
            </li>
            <li>
              <strong>Dice Coefficient (F1):</strong> Measures pixel
              classification accuracy, harmonizing precision and recall.
            </li>
            <li>
              <strong>Boundary F-Score:</strong> Measures edge/contour accuracy.
              Similar formula to Dice Coefficient but focuses strictly on the
              precision of the boundary pixels, which is critical for clean
              splitting.
            </li>
          </ul>

          <h4>Evaluated Models</h4>
          <p>
            We benchmarked four state-of-the-art architectures to select our
            backbone:
          </p>
          <ul>
            <li>
              <strong
                ><a
                  href="https://huggingface.co/facebook/mask2former-swin-large-ade-semantic"
                  target="_blank"
                  >Mask2Former (Swin-Large)</a
                ></strong
              >: A transformer-based model that treats segmentation as a mask
              classification problem. <em>Why:</em> Known for state-of-the-art
              performance and sharp boundary detection.
            </li>
            <li>
              <strong
                ><a
                  href="https://huggingface.co/shi-labs/oneformer_ade20k_swin_large"
                  target="_blank"
                  >OneFormer</a
                ></strong
              >: A universal image segmentation framework that unifies semantic,
              instance, and panoptic segmentation. <em>Why:</em> To see if a
              multi-task model offers better context understanding for complex
              indoor scenes.
            </li>
            <li>
              <strong
                ><a
                  href="https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512"
                  target="_blank"
                  >SegFormer</a
                ></strong
              >: A lightweight, efficient transformer design. <em>Why:</em> To
              test if a faster, lighter model could provide sufficient accuracy
              for real-time applications.
            </li>
            <li>
              <strong>NYU Depth V2 (via SegFormer)</strong>: Models fine-tuned
              on the NYU Depth dataset. <em>Why:</em> To evaluate if models
              trained specifically on indoor depth datasets generalize better to
              wall geometry than general datasets like ADE20K.
            </li>
          </ul>
        </div>
        <div class="content-block">
          <h3>Qualitative Results</h3>
          <h4>Box Plot Comparison of the 4 Models</h4>
          <div class="boxplot-container">
            <figure class="boxplot-figure">
              <img
                src="images/box_plot_comparison.png"
                alt="Box Plot Comparison: Mask2Former, OneFormer, SegFormer, and NYU"
                class="boxplot-image"
              />
              <figcaption class="boxplot-caption">
                Fig. Box Plot Comparison: Performance metrics across Mask2Former, OneFormer, SegFormer, and NYU models
              </figcaption>
            </figure>
          </div>
          <p><strong>Discussion: </strong>we can see from the graph that the performance of Mask2Former and OneFormer and similar, both slightly outperforms the SegFormer and the NYU model</p>
          <hr>
          <br>
          <p><strong>Statistics Summary of the 4 Models</strong></p>
          <div class="boxplot-container">
            <figure class="boxplot-figure">
              <img
                src="images/statistics_summary.png"
                alt="Statistics Summary: Mask2Former, OneFormer, SegFormer, and NYU"
                class="boxplot-image"
              />
              <figcaption class="boxplot-caption">
                Fig. Box Plot Comparison: Performance metrics across Mask2Former, OneFormer, SegFormer, and NYU models
              </figcaption>
            </figure>
          </div>
          <p><strong>Discussion:</strong> We could see from this graph that the OneFormer cost more computation/time than any other models. This is one reason that we eventually chose the Mask2Former as the segmentation model.</p>
          <hr>
          <br>
          
          <h3>Output Results</h3>
          <p>Our result using ADE_val_00000130 from ADE20k validation set</p>
          <div class="boxplot-container">
            <figure class="boxplot-figure">
              <img
                src="images/ADE_val_00000130_visualization.png"
                alt="ADE_val_00000130_visualization"
                class="boxplot-image"
              />
              <figcaption class="boxplot-caption">
                Fig. ADE_val_00000130_visualization
              </figcaption>
            </figure>
            <p style="text-align: center; margin-top: 15px; font-size: 1rem;">
              <strong>IoU:</strong> 0.5166 | 
              <strong>Dice:</strong> 0.6841 | 
              <strong>Boundary F-score:</strong> 0.2569
            </p>
          </div>

          <div class="boxplot-container">
            <figure class="boxplot-figure">
              <img
                src="images/ade_132_val_visualization.png"
                alt="ade_132_val_visualization"
                class="boxplot-image"
              />
              <figcaption class="boxplot-caption">
                Fig. ade_132_val_visualization
              </figcaption>
            </figure>
            <p style="text-align: center; margin-top: 15px; font-size: 1rem;">
              <strong>IoU:</strong> 0.8771 | 
              <strong>Dice:</strong> 0.9345 | 
              <strong>Boundary F-score:</strong> 0.4231
            </p>
          </div>

          <h3>Quantitative Evaluation</h3>
          <p>
            We evaluated our segmentation and splitting accuracy against
            manually annotated ground truth.
          </p>

          <table class="results-table">
            <thead>
              <tr>
                <th>Method</th>
                <th>IoU (Intersection over Union)</th>
                <th>Boundary F-Score</th>
                <th>Inference Time (s)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Baseline (Color Clustering)</td>
                <td>0.35</td>
                <td>N/A</td>
                <td>~0.05</td>
              </tr>
              <tr>
                <td><strong>Ours (Mask2Former + Refiner)</strong></td>
                <td><strong>0.6075</strong></td>
                <td><strong>0.3842</strong></td>
                <td><strong>0.1113</strong></td>
              </tr>
            </tbody>
          </table>

          <h3>Comparison</h3>
          <div class="placeholder-image">
            <img
              src="images/mapping.png"
              alt="Comparison of Our Method vs. Naive Homography"
              style="
                width: 100%;
                border-radius: 8px;
                box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
              "
            />
            <p><em>Fig 4. Visual Result demonstrating our method's ability to handle occlusion and perspective (compared to naive homography which fails on occlusions)</em></p>
          </div>

        </div>
      </section>

      <!-- Discussion -->
      <section id="discussion">
        <h2>Discussion</h2>
        <div class="content-block">
          <h3>Findings & Implications</h3>
          <p>
            Our results demonstrate that while modern semantic segmentation is
            powerful, it lacks geometric understanding. The hybrid approach of
            <strong
              >Deep Learning (for semantics) + Classical CV (for
              geometry)</strong
            >
            proved superior to using either approach in isolation.
          </p>
          <br>

          <p>
            One thing we noticed, for example, from our Results Section the two output images,
            we noticed that one image has lower IoU, Dice Score, and Boundary F-score, but overlays 
            well, thus the final outcome is good. In contract, the other has higher scores of all 3 metrics, 
            but the final outcomes is not as well, which is due to the overlay effects. This implies that the 
            final outcome don't solely depend on one technique, but both performing well.
          </p>

          <h3>Limitations & Challenges</h3>
          <ul>
            <li>
              <strong>Occlusions:</strong> Furniture blocking the wall-floor
              boundary can sometimes confuse the polygon approximation.
            </li>
            <li>
              <strong>Complex Geometry:</strong> Non-planar walls (curved
              surfaces) are currently modeled as flat planes.
            </li>
          </ul>

          <h3>Future Work</h3>
          <p>
            Future extensions could include implementing
            <strong>PlaneRCNN</strong> for 3D-aware segmentation or integrating
            a monocular depth estimation model (like MiDaS) to handle occlusions
            more accurately.
          </p>
        </div>
      </section>

      <section id="archived-reports">
        <div class="content-block">
          <p style="font-size: 0.9em; color: #666;">
            <strong>Archived Reports:</strong> View our 
            <a href="midterm.html">Midterm Project Report</a> 
            for details on initial experiments and methodology evolution.
          </p>
        </div>
      </section>
    </main>

    <footer>
      <div class="container">
        <p>&copy; 2025 Wall Designer Tool. CS566 Final Project.</p>
      </div>
    </footer>
  </body>
</html>
